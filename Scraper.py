import requests
import json
import time
import re
import os

regular_expressions = {
    "Base64_pe": "^TV(oA|pB|pQ|qA|qQ|ro)\w+",
    "pe_file": "^(MZ)",
    "pe_file_hex": "^(4d5a)",
    "SqlMap": "Usage of sqlmap",
    "Python_systemcall": "os.system",
    "Bash_Script": "#!/bin/bash",
    "VBA": "Auto_Open\(\)",
    "VBS": "wscript.shell",
    "Python_Script": "#!/usr/bin/python",
    "Hidden_Powershell": "-WindowStyle hidden",
    "Powershell_ExecutionPolicy": "-ExecutionPolicy ByPass",
    "Powershell_downloadstring": "\.DownloadString\(",
    "Powershell_executable": "powershell.exe",
    "PowerShell": "powershell",
    "PHP_base64": "base64_decode\(",
    "Javascript_eval": "eval\(",
    "c_srtcpy": "strcpy\("
}

found = 0
repeat = 0

inital_set = 50
loop_set = 50
delay = 60

s = requests.Session()

scraped_urls = []

response = s.get("https://scrape.pastebin.com/api_scraping.php?limit=" + str(inital_set))
data = json.loads(response.text)

for _ in range(100):
    repeat = 0
    current_urls = []
    for i in data:
        if i['scrape_url'][50:] not in scraped_urls: # not already scraped, download data, add to scraped list
            current_urls.append(i['scrape_url'][50:])
            paste_data = s.get((i['scrape_url']))
            paste_data = paste_data.text

            for j in regular_expressions:
                if re.search(regular_expressions[j], paste_data) is not None:
                    found += 1
                    print(j + " Found")
                    # print(paste_data)

                    file_name = j + " " + str((time.time())) + ".txt"

                    f = open(os.getcwd() + "\\files\\" + file_name, "w+", encoding="utf-8")
                    f.write(paste_data)
                    f.close()

                    print('--------------------------------------------------------------------------------------')

        else:
            repeat += 1

    scraped_urls.extend(current_urls)
    time.sleep(delay)

    # print(str(found) + ' items')
    print(str(repeat) + ' repeated data')

    print('-----------------------')

    if len(scraped_urls) > 250:
        scraped_urls = scraped_urls[100:] # remove oldest urls if too long

    response = s.get("https://scrape.pastebin.com/api_scraping.php?limit=" + str(loop_set))
    data = json.loads(response.text)

print("size of url: " + str(len(scraped_urls)))
print("Found " + str(found) + " items of interest")
